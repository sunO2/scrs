use async_trait::async_trait;
use reqwest::Client;
use std::time::{Duration, Instant};
use std::sync::{Arc, Mutex as StdMutex};
use tokio::sync::RwLock;
use tracing::{debug, info, warn, error};
use tokio_stream::StreamExt;
use crate::agent::core::traits::{ModelClient, ModelResponse, ModelError, ModelInfo, ChatMessage, MessageRole};
use crate::agent::llm::types::{ChatRequest, ModelConfig, MessageContent, ChatMessage as ApiChatMessage, MessageRole as ApiMessageRole};
use crate::agent::llm::prompts;
use crate::agent::logger::{AgentLogger, LogMessage};
use serde::{Deserialize, Serialize};

// å¯¼å…¥ ActionEnum ç”¨äºè§£æå“åº”
use crate::agent::actions::base::ActionEnum;

/// AutoGLM æµå¼å“åº”çš„å¢é‡æ•°æ®
#[derive(Debug, Deserialize)]
#[serde(tag = "type")]
enum StreamEvent {
    #[serde(rename = "token")]
    Token { token: String },
    #[serde(rename = "message_end")]
    MessageEnd,
}

/// AutoGLM æ€§èƒ½æŒ‡æ ‡
#[derive(Debug, Clone, Serialize)]
pub struct PerformanceMetrics {
    /// é¦–ä¸ª token æ—¶é—´ï¼ˆç§’ï¼‰
    pub time_to_first_token: Option<f64>,
    /// æ€è€ƒç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    pub time_to_thinking_end: Option<f64>,
    /// æ€»æ¨ç†æ—¶é—´ï¼ˆç§’ï¼‰
    pub total_time: f64,
}

/// AutoGLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒæµå¼å“åº”å’Œç‰¹æ®Šæ ‡è®°è§£æ
pub struct AutoGLMClient {
    /// ä¸»å®¢æˆ·ç«¯ï¼Œç”¨äºä¸»è¦æ“ä½œå†³ç­–
    client: Client,
    /// è¾…åŠ©å®¢æˆ·ç«¯ï¼Œç”¨äºä¿®æ­£å’Œè§„èŒƒåŒ–ä¸»æ¨¡å‹çš„è¾“å‡º
    auxiliary_client: Client,
    /// æ¨¡å‹é…ç½®
    config: ModelConfig,
    /// æ—¥å¿—è®°å½•å™¨ï¼ˆä½¿ç”¨ std::sync::Mutex ä»¥æ”¯æŒåŒæ­¥è®¿é—®ï¼‰
    logger: Arc<StdMutex<Option<Arc<AgentLogger>>>>,
}

impl AutoGLMClient {
    /// åˆ›å»ºæ–°çš„ AutoGLM å®¢æˆ·ç«¯
    pub fn new(config: ModelConfig) -> Result<Self, ModelError> {
        info!("åˆ›å»º AutoGLM å®¢æˆ·ç«¯: {}", config.model_name);
        info!("  API ç«¯ç‚¹: {}", config.base_url);
        info!("  è¶…æ—¶æ—¶é—´: {}s", config.timeout);
        info!("  API Key: {}...", &config.api_key[..config.api_key.len().min(10)]);

        // æ˜¾ç¤ºè¾…åŠ©æ¨¡å‹é…ç½®
        if let Some(ref aux_name) = config.auxiliary_model_name {
            info!("  è¾…åŠ©æ¨¡å‹: {}", aux_name);
        } else {
            info!("  æœªé…ç½®è¾…åŠ©æ¨¡å‹");
        }

        let client = Client::builder()
            .timeout(Duration::from_secs(config.timeout))
            .connect_timeout(Duration::from_secs(30))
            .pool_idle_timeout(Duration::from_secs(120))
            .tcp_keepalive(Duration::from_secs(600))
            .build()
            .map_err(|e| ModelError::ApiError(format!("åˆ›å»º HTTP å®¢æˆ·ç«¯å¤±è´¥: {}", e)))?;

        // åˆ›å»ºè¾…åŠ©å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ç›¸åŒçš„é…ç½®ï¼‰
        let auxiliary_client = Client::builder()
            .timeout(Duration::from_secs(config.timeout))
            .connect_timeout(Duration::from_secs(30))
            .pool_idle_timeout(Duration::from_secs(120))
            .tcp_keepalive(Duration::from_secs(600))
            .build()
            .map_err(|e| ModelError::ApiError(format!("åˆ›å»ºè¾…åŠ© HTTP å®¢æˆ·ç«¯å¤±è´¥: {}", e)))?;

        Ok(Self {
            client,
            auxiliary_client,
            config,
            logger: Arc::new(StdMutex::new(None)),
        })
    }

    /// è®°å½• API å¯¹è¯åˆ°æ—¥å¿—æ–‡ä»¶
    async fn log_api_call(
        &self,
        stage: &str,
        messages: Vec<LogMessage>,
        model_response: &str,
        duration_ms: u64,
    ) {
        // å…‹éš† logger å¼•ç”¨ä»¥é¿å…åœ¨ await æ—¶æŒæœ‰é”
        let logger = self.logger.lock().unwrap().clone();
        if let Some(logger) = logger {
            let _ = logger.log_action(
                0, // step_number
                messages,
                None, // screenshot
                model_response.to_string(), // ç›´æ¥è®°å½•åŸå§‹å“åº”
                None, // thinking
                stage.to_string(),
                serde_json::json!({ "duration_ms": duration_ms }),
                None, // action_result
                0, // tokens_used
                duration_ms,
            ).await;
        }
    }

    /// å‘é€æµå¼èŠå¤©è¯·æ±‚
    async fn send_stream_request(&self, request: ChatRequest) -> Result<String, ModelError> {
        let url = format!("{}/chat/completions", self.config.base_url);

        debug!("å‘é€ AutoGLM æµå¼è¯·æ±‚åˆ°: {}", url);

        let mut stream_request = request.clone();
        stream_request.stream = Some(true);

        let response = self
            .client
            .post(&url)
            .header("Authorization", format!("Bearer {}", self.config.api_key))
            .header("Content-Type", "application/json")
            .json(&stream_request)
            .send()
            .await
            .map_err(|e| ModelError::NetworkError(format!("å‘é€è¯·æ±‚å¤±è´¥: {}", e)))?;

        let status = response.status();

        if !status.is_success() {
            let error_text = response
                .text()
                .await
                .unwrap_or_else(|_| "æ— æ³•è¯»å–é”™è¯¯å“åº”".to_string());

            error!("AutoGLM è¯·æ±‚å¤±è´¥: {} - {}", status, error_text);

            if status.as_u16() == 401 {
                return Err(ModelError::InvalidApiKey);
            }

            if status.as_u16() == 429 {
                return Err(ModelError::RateLimit);
            }

            return Err(ModelError::ApiError(format!(
                "è¯·æ±‚å¤±è´¥: {} - {}",
                status, error_text
            )));
        }

        // å¤„ç†æµå¼å“åº”
        let mut full_content = String::new();
        let mut byte_stream = response.bytes_stream();

        while let Some(chunk_result) = byte_stream.next().await {
            let chunk = chunk_result
                .map_err(|e| ModelError::NetworkError(format!("è¯»å–æµæ•°æ®å¤±è´¥: {}", e)))?;

            let chunk_str = String::from_utf8_lossy(&chunk);
            full_content.push_str(&chunk_str);
        }

        Ok(full_content)
    }

    /// å‘é€éæµå¼èŠå¤©è¯·æ±‚
    async fn send_request(&self, request: ChatRequest) -> Result<ChatResponse, ModelError> {
        let url = format!("{}/chat/completions", self.config.base_url);

        info!("å‘é€ AutoGLM è¯·æ±‚åˆ°: {}", url);
        info!("  æ¨¡å‹: {}", request.model);
        info!("  æ¶ˆæ¯æ•°: {}", request.messages.len());

        // å‘é€è¯·æ±‚å¹¶å¤„ç†é”™è¯¯
        match self._send_request(&url, &request, &self.client, &self.config.api_key).await {
            Ok(response) => Ok(response),
            Err(e) => {
                error!("AutoGLM è¯·æ±‚å¤±è´¥: {}", e);
                error!("è¯·æ£€æŸ¥:");
                error!("  1. API Key æ˜¯å¦æ­£ç¡®è®¾ç½®");
                error!("  2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸");
                error!("  3. API ç«¯ç‚¹æ˜¯å¦å¯è®¿é—®: {}", self.config.base_url);
                error!("  4. æ˜¯å¦æœ‰è¶³å¤Ÿçš„é…é¢");
                Err(e)
            }
        }
    }

    /// ä½¿ç”¨è¾…åŠ©æ¨¡å‹å‘é€è¯·æ±‚ä»¥ä¿®æ­£å“åº”
    async fn send_auxiliary_request(&self, original_content: &str) -> Result<String, ModelError> {
        // å¦‚æœæ²¡æœ‰é…ç½®è¾…åŠ©æ¨¡å‹åç§°ï¼Œç›´æ¥è¿”å›åŸå§‹å†…å®¹
        let aux_model_name = match &self.config.auxiliary_model_name {
            Some(name) => name,
            None => {
                debug!("æœªé…ç½®è¾…åŠ©æ¨¡å‹ï¼Œè·³è¿‡å“åº”ä¿®æ­£");
                return Ok(original_content.to_string());
            }
        };

        info!("ä½¿ç”¨è¾…åŠ©æ¨¡å‹ä¿®æ­£å“åº”: {}", aux_model_name);

        let url = format!("{}/chat/completions", self.config.base_url);

        // æ„å»ºè¾…åŠ©æ¨¡å‹è¯·æ±‚
        let system_prompt = prompts::get_auxiliary_system_prompt();
        let user_message = format!("è¯·ä¿®æ­£ä»¥ä¸‹è¾“å‡ºï¼Œä½¿å…¶ç¬¦åˆæ ¼å¼è¦æ±‚ï¼š\n\n{}", original_content);

        let api_messages = vec![
            ApiChatMessage {
                role: ApiMessageRole::System,
                content: MessageContent::Text(system_prompt),
            },
            ApiChatMessage {
                role: ApiMessageRole::User,
                content: MessageContent::Text(user_message),
            },
        ];

        let request = ChatRequest {
            model: aux_model_name.clone(),
            messages: api_messages,
            max_tokens: Some(2048),
            temperature: Some(0.0),
            top_p: Some(0.85),
            stream: Some(false),
        };

        let chat_response = self._send_request(&url, &request, &self.auxiliary_client, &self.config.api_key).await?;

        // æå–ä¿®æ­£åçš„å†…å®¹
        let choice = chat_response.choices.first().ok_or_else(|| {
            ModelError::ParseError("è¾…åŠ©æ¨¡å‹å“åº”ä¸­æ²¡æœ‰é€‰æ‹©é¡¹".to_string())
        })?;

        let corrected_content = match &choice.message.content {
            MessageContent::Text(text) => text.clone(),
            _ => original_content.to_string(),
        };

        info!("è¾…åŠ©æ¨¡å‹ä¿®æ­£å®Œæˆ");
        debug!("åŸå§‹å†…å®¹: {}", original_content);
        debug!("ä¿®æ­£åå†…å®¹: {}", corrected_content);

        Ok(corrected_content)
    }

    async fn _send_request(
        &self,
        url: &str,
        request: &ChatRequest,
        client: &Client,
        api_key: &str,
    ) -> Result<ChatResponse, ModelError> {
        // æ‰“å°è¯·æ±‚è¯¦æƒ…ï¼ˆé€‰æ‹©æ€§è¾“å‡ºï¼Œè¿‡æ»¤å›¾ç‰‡æ•°æ®ï¼‰
        info!("========== AutoGLM è¯·æ±‚ ==========");
        info!("URL: {}", url);
        info!("æ¨¡å‹: {}", request.model);
        info!("å‚æ•°: max_tokens={:?}, temperature={:?}, top_p={:?}, stream={:?}",
            request.max_tokens, request.temperature, request.top_p, request.stream);
        info!("æ¶ˆæ¯æ•°é‡: {}", request.messages.len());
        info!("================================");

        let response = client
            .post(url)
            .header("Authorization", format!("Bearer {}", api_key))
            .header("Content-Type", "application/json")
            .json(request)
            .send()
            .await
            .map_err(|e| {
                error!("ğŸ”´ AutoGLM ç½‘ç»œè¯·æ±‚å¤±è´¥");
                error!("   URL: {}", url);
                error!("   é”™è¯¯ç±»å‹: {:?}", e);

                // æä¾›æ›´è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
                if e.is_timeout() {
                    error!("   é”™è¯¯: è¯·æ±‚è¶…æ—¶");
                    error!("   å¯èƒ½çš„åŸå› :");
                    error!("   1. ç½‘ç»œè¿æ¥ä¸ç¨³å®š");
                    error!("   2. API æœåŠ¡å™¨å“åº”ç¼“æ…¢");
                    error!("   3. è¯·æ±‚å¤ªå¤§ï¼Œå¤„ç†æ—¶é—´è¿‡é•¿");
                    error!("   å»ºè®®:");
                    error!("   - æ£€æŸ¥ç½‘ç»œè¿æ¥");
                    error!("   - å¢åŠ  timeout æ—¶é—´");
                    error!("   - å‡å°è¯·æ±‚å¤§å°ï¼ˆå¦‚å‡å°‘å›¾ç‰‡æ•°é‡ï¼‰");
                } else if e.is_connect() {
                    error!("   é”™è¯¯: æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨");
                    error!("   å¯èƒ½çš„åŸå› :");
                    error!("   1. ç½‘ç»œæœªè¿æ¥");
                    error!("   2. API æœåŠ¡å™¨åœ°å€é”™è¯¯: {}", url);
                    error!("   3. é˜²ç«å¢™æˆ–ä»£ç†é˜»æ­¢è¿æ¥");
                    error!("   4. DNS è§£æå¤±è´¥");
                    error!("   å»ºè®®:");
                    error!("   - æ£€æŸ¥ç½‘ç»œè¿æ¥");
                    error!("   - éªŒè¯ API URL æ˜¯å¦æ­£ç¡®");
                    error!("   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®");
                    error!("   - å°è¯•ä½¿ç”¨ VPN");
                } else {
                    error!("   å…¶ä»–ç½‘ç»œé”™è¯¯");
                    error!("   åŸå§‹é”™è¯¯: {}", e);
                }

                ModelError::NetworkError(format!("å‘é€è¯·æ±‚å¤±è´¥: {}", e))
            })?;

        let status = response.status();
        debug!("å“åº”çŠ¶æ€: {}", status);

        let response_text = response
            .text()
            .await
            .map_err(|e| ModelError::NetworkError(format!("è¯»å–å“åº”å¤±è´¥: {}", e)))?;

        // æ‰“å°å“åº”è¯¦æƒ…
        info!("========== AutoGLM å“åº” ==========");
        info!("çŠ¶æ€ç : {}", status);
        info!("å“åº”ä½“ ({} å­—èŠ‚):", response_text.len());
        info!("================================");

        if !status.is_success() {
            warn!("AutoGLM è¯·æ±‚å¤±è´¥: {} - {}", status, response_text);

            if status.as_u16() == 401 {
                error!("API Key æ— æ•ˆ");
                return Err(ModelError::InvalidApiKey);
            }

            if status.as_u16() == 429 {
                error!("è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè§¦å‘é™æµ");
                return Err(ModelError::RateLimit);
            }

            return Err(ModelError::ApiError(format!(
                "è¯·æ±‚å¤±è´¥: {} - {}",
                status, response_text
            )));
        }

        let chat_response: ChatResponse = serde_json::from_str(&response_text).map_err(|e| {
            warn!("è§£æ AutoGLM å“åº”å¤±è´¥: {}", e);
            warn!("å“åº”å†…å®¹: {}", &response_text);
            ModelError::ParseError(format!("è§£æå“åº”å¤±è´¥: {}", e))
        })?;

        Ok(chat_response)
    }

    /// è§£æ AutoGLM å“åº”ï¼ˆä½¿ç”¨ ActionEnum çš„é€šç”¨è§£ææ–¹æ³•ï¼‰
    fn parse_response(&self, content: &str) -> (Option<String>, Vec<ActionEnum>) {
        ActionEnum::parse_from_response(content)
    }

    /// é˜¶æ®µ1: å¤§æ¨¡å‹è§„åˆ’åŠ¨ä½œ
    /// ç”¨äºä¸‰é˜¶æ®µæ¨¡å¼ï¼Œå¤§æ¨¡å‹ä½œä¸ºæé—®è€…ï¼Œå‘æ‰§è¡ŒåŠ©æ‰‹æå‡ºæ“ä½œè¯·æ±‚ï¼ˆä¸éœ€è¦æˆªå›¾ï¼‰
    async fn plan_action(
        &self,
        messages: Vec<ChatMessage>,
    ) -> Result<String, ModelError> {
        let planning_model = self.config.planning_model_name
            .as_ref()
            .or(self.config.auxiliary_model_name.as_ref())
            .ok_or_else(|| ModelError::ApiError("æœªé…ç½®è§„åˆ’æ¨¡å‹".to_string()))?;

        info!("=== é˜¶æ®µ1: å¤§æ¨¡å‹è§„åˆ’ï¼ˆæé—®è€…ï¼‰ ===");
        info!("ä½¿ç”¨æ¨¡å‹: {}", planning_model);

        let start_time = Instant::now();

        // è½¬æ¢å¯¹è¯å†å²ï¼ˆagent.rs å·²ç»åŒ…å«äº†ç³»ç»Ÿæç¤ºè¯ï¼Œä¸éœ€è¦å†æ·»åŠ ï¼‰
        let api_messages: Vec<ApiChatMessage> = messages.iter().map(|msg| {
            let role = match msg.role {
                MessageRole::System => ApiMessageRole::System,
                MessageRole::User => ApiMessageRole::User,
                MessageRole::Assistant => ApiMessageRole::Assistant,
            };

            ApiChatMessage {
                role,
                content: MessageContent::Text(msg.content.clone()),
            }
        }).collect();

        let request = ChatRequest {
            model: planning_model.to_string(),
            messages: api_messages.clone(),
            max_tokens: Some(1024),
            temperature: Some(0.3),
            top_p: Some(0.1),
            stream: Some(false),
        };

        let chat_response = self._send_request(
            &format!("{}/chat/completions", self.config.base_url),
            &request,
            &self.client,
            &self.config.api_key
        ).await?;

        let choice = chat_response.choices.first().ok_or_else(|| {
            ModelError::ParseError("è§„åˆ’æ¨¡å‹å“åº”ä¸­æ²¡æœ‰é€‰æ‹©é¡¹".to_string())
        })?;

        let planning_output = match &choice.message.content {
            MessageContent::Text(text) => text.clone(),
            _ => return Err(ModelError::ParseError("è§„åˆ’æ¨¡å‹è¾“å‡ºæ ¼å¼é”™è¯¯".to_string())),
        };

        let duration = start_time.elapsed().as_millis() as u64;
        info!("è§„åˆ’è¯·æ±‚: {} (è€—æ—¶: {}ms)", planning_output, duration);

        // è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆå¯¹è¯æ ¼å¼ï¼‰
        let log_messages: Vec<LogMessage> = api_messages.iter().map(|msg| {
            LogMessage {
                role: format!("{:?}", msg.role).to_lowercase(),
                content: match &msg.content {
                    MessageContent::Text(text) => text.clone(),
                    _ => "".to_string(),
                },
            }
        }).collect();

        self.log_api_call(
            "planning",
            log_messages,
            &planning_output,
            duration,
        ).await;

        Ok(planning_output)
    }

    /// é˜¶æ®µ2: å°æ¨¡å‹ç”Ÿæˆå…·ä½“åŠ¨ä½œ
    /// ç”¨äºä¸‰é˜¶æ®µæ¨¡å¼ï¼Œå°æ¨¡å‹æ ¹æ®åŠ¨ä½œæè¿°ç”Ÿæˆå…·ä½“æ‰§è¡Œå‚æ•°
    async fn execute_plan(
        &self,
        action_description: &str,
        screenshot: &str,
        screen_width: u32,
        screen_height: u32,
    ) -> Result<String, ModelError> {
        let execution_model = self.config.execution_model_name
            .as_ref()
            .unwrap_or(&self.config.model_name);

        info!("=== é˜¶æ®µ2: å°æ¨¡å‹æ‰§è¡Œ ===");
        info!("ä½¿ç”¨æ¨¡å‹: {}", execution_model);
        info!("åŠ¨ä½œæè¿°: {}", action_description);

        let start_time = Instant::now();

        // æ„å»ºæ‰§è¡Œè¯·æ±‚ï¼ˆåªç»™æè¿°+æˆªå›¾ï¼Œä¸ç»™å†å²ï¼‰
        let system_prompt = prompts::get_execution_system_prompt(screen_width, screen_height);
        let user_message = format!(
            "{}\n\nè¯·æ ¹æ®å½“å‰æˆªå›¾å’Œä¸Šè¿°åŠ¨ä½œæè¿°ç”Ÿæˆå…·ä½“çš„æ‰§è¡Œå‚æ•°ã€‚",
            action_description
        );

        let api_messages = vec![
            ApiChatMessage {
                role: ApiMessageRole::System,
                content: MessageContent::Text(system_prompt),
            },
            ApiChatMessage {
                role: ApiMessageRole::User,
                content: MessageContent::Multimodal(vec![
                    crate::agent::llm::types::ContentBlock {
                        block_type: "image_url".to_string(),
                        text: None,
                        image_url: Some(crate::agent::llm::types::ImageUrl::from_base64(screenshot)),
                    },
                    crate::agent::llm::types::ContentBlock {
                        block_type: "text".to_string(),
                        text: Some(user_message.clone()),
                        image_url: None,
                    },
                ]),
            },
        ];

        let request = ChatRequest {
            model: execution_model.to_string(),
            messages: api_messages.clone(),
            max_tokens: Some(2048),
            temperature: Some(0.1),
            top_p: Some(0.1),
            stream: Some(false),
        };

        let chat_response = self._send_request(
            &format!("{}/chat/completions", self.config.base_url),
            &request,
            &self.client,
            &self.config.api_key
        ).await?;

        let choice = chat_response.choices.first().ok_or_else(|| {
            ModelError::ParseError("æ‰§è¡Œæ¨¡å‹å“åº”ä¸­æ²¡æœ‰é€‰æ‹©é¡¹".to_string())
        })?;

        let execution_output = match &choice.message.content {
            MessageContent::Text(text) => text.clone(),
            _ => return Err(ModelError::ParseError("æ‰§è¡Œæ¨¡å‹è¾“å‡ºæ ¼å¼é”™è¯¯".to_string())),
        };

        let duration = start_time.elapsed().as_millis() as u64;
        info!("æ‰§è¡Œè¾“å‡º: {} (è€—æ—¶: {}ms)", execution_output, duration);

        // è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆå¯¹è¯æ ¼å¼ï¼‰
        let log_messages = vec![
            LogMessage {
                role: "system".to_string(),
                content: "[ç³»ç»Ÿæç¤ºè¯å·²çœç•¥]".to_string(),
            },
            LogMessage {
                role: "user".to_string(),
                content: user_message,
            },
        ];

        self.log_api_call(
            "execution",
            log_messages,
            &execution_output,
            duration,
        ).await;

        Ok(execution_output)
    }

    /// å†…éƒ¨ä¸‰é˜¶æ®µå¤„ç†
    /// å®Œæ•´çš„ä¸‰é˜¶æ®µæµç¨‹ï¼šå¤§æ¨¡å‹è§„åˆ’(æé—®) -> å°æ¨¡å‹æ‰§è¡Œ(ç­”é¢˜) -> å¤§æ¨¡å‹ä¿®æ­£
    async fn process_three_stage_internal(
        &self,
        messages: Vec<ChatMessage>,
        screenshot: &str,
        screen_width: u32,
        screen_height: u32,
    ) -> Result<ModelResponse, ModelError> {
        let start_time = Instant::now();

        // é˜¶æ®µ1: å¤§æ¨¡å‹è§„åˆ’ï¼ˆä¸éœ€è¦æˆªå›¾ï¼Œä½œä¸ºæé—®è€…ï¼‰
        let planning_request = self.plan_action(messages.clone()).await?;
        info!("è§„åˆ’ç»“æœ: {}", planning_request);

        // é˜¶æ®µ2: å°æ¨¡å‹æ‰§è¡Œï¼ˆéœ€è¦æˆªå›¾ï¼Œä½œä¸ºç­”é¢˜è€…ï¼‰
        let mut content = self.execute_plan(
            &planning_request,
            screenshot,
            screen_width,
            screen_height
        ).await?;

        // å°è¯•è§£æ
        let (thinking, parsed_actions) = self.parse_response(&content);

        // é˜¶æ®µ3: å¤§æ¨¡å‹ä¿®æ­£ï¼ˆå¦‚æœè§£æå¤±è´¥ï¼‰
        if parsed_actions.is_empty() {
            info!("è§£æå¤±è´¥ï¼Œè¿›å…¥é˜¶æ®µ3: å¤§æ¨¡å‹ä¿®æ­£");
            match self.send_auxiliary_request(&content).await {
                Ok(corrected_content) => {
                    content = corrected_content;
                    info!("ä¿®æ­£å®Œæˆ");
                },
                Err(e) => {
                    error!("ä¿®æ­£å¤±è´¥: {}", e);
                    return Err(e);
                }
            }
        }

        let total_time = start_time.elapsed().as_secs_f64();

        // æœ€ç»ˆè§£æ
        let (thinking, parsed_actions) = self.parse_response(&content);

        info!("ğŸ“Š ä¸‰é˜¶æ®µæ€§èƒ½æŒ‡æ ‡:");
        info!("   æ€»æ¨ç†æ—¶é—´: {:.3}s", total_time);
        info!("   è§£æåˆ°çš„æ“ä½œæ•°: {}", parsed_actions.len());

        Ok(ModelResponse {
            content,
            actions: parsed_actions,
            confidence: 0.8,
            reasoning: thinking,
            tokens_used: 0, // ä¸‰é˜¶æ®µæ¨¡å¼éœ€è¦å•ç‹¬è®¡ç®—
        })
    }
}

#[async_trait]
impl ModelClient for AutoGLMClient {
    async fn query_with_messages(
        &self,
        messages: Vec<ChatMessage>,
        screenshot: Option<&str>,
    ) -> Result<ModelResponse, ModelError> {
        debug!("æŸ¥è¯¢ AutoGLMï¼Œæ¶ˆæ¯æ•°é‡: {}", messages.len());

        // å¦‚æœå¯ç”¨ä¸‰é˜¶æ®µæ¨¡å¼ï¼Œä½¿ç”¨ä¸‰é˜¶æ®µæµç¨‹
        if self.config.enable_three_stage {
            let screenshot = screenshot.ok_or_else(|| {
                ModelError::ParseError("ä¸‰é˜¶æ®µæ¨¡å¼éœ€è¦æˆªå›¾".to_string())
            })?;

            // TODO: ä»è®¾å¤‡è·å–å±å¹•å°ºå¯¸ï¼Œè¿™é‡Œå…ˆä½¿ç”¨å›ºå®šå€¼
            // åç»­å¯ä»¥é€šè¿‡å‚æ•°ä¼ å…¥æˆ–ä»é…ç½®è¯»å–
            let screen_width = 1080;
            let screen_height = 2400;

            info!("å¯ç”¨ä¸‰é˜¶æ®µæ¨¡å¼");
            return self.process_three_stage_internal(
                messages,
                screenshot,
                screen_width,
                screen_height
            ).await;
        }

        // å¦åˆ™ä½¿ç”¨åŸæœ‰çš„å•é˜¶æ®µæµç¨‹
        let start_time = Instant::now();

        // è½¬æ¢æ¶ˆæ¯æ ¼å¼
        let mut api_messages = vec![];

        // æ‰¾åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•ï¼ˆç”¨äºæ·»åŠ æˆªå›¾ï¼‰
        let last_user_msg_index = messages.iter().rposition(|msg| {
            matches!(msg.role, MessageRole::User)
        });

        for (idx, msg) in messages.iter().enumerate() {
            let role = match msg.role {
                MessageRole::System => ApiMessageRole::System,
                MessageRole::User => ApiMessageRole::User,
                MessageRole::Assistant => ApiMessageRole::Assistant,
            };

            // åªåœ¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸­æ·»åŠ æˆªå›¾
            let is_last_user_msg = last_user_msg_index == Some(idx);

            let content = if is_last_user_msg && screenshot.is_some() {
                MessageContent::Multimodal(vec![
                    crate::agent::llm::types::ContentBlock {
                        block_type: "image_url".to_string(),
                        text: None,
                        image_url: Some(crate::agent::llm::types::ImageUrl::from_base64(screenshot.unwrap())),
                    },
                    crate::agent::llm::types::ContentBlock {
                        block_type: "text".to_string(),
                        text: Some(msg.content.clone()),
                        image_url: None,
                    },
                ])
            } else {
                MessageContent::Text(msg.content.clone())
            };

            api_messages.push(ApiChatMessage { role, content });
        }

        // æ„å»ºè¯·æ±‚
        let request = ChatRequest {
            model: self.config.model_name.clone(),
            messages: api_messages,
            max_tokens: Some(self.config.max_tokens),
            temperature: Some(self.config.temperature),
            top_p: Some(self.config.top_p),
            stream: Some(false),
        };

        // å‘é€è¯·æ±‚
        let chat_response = self.send_request(request).await?;

        // è§£æå“åº”
        let choice = chat_response.choices.first().ok_or_else(|| {
            ModelError::ParseError("å“åº”ä¸­æ²¡æœ‰é€‰æ‹©é¡¹".to_string())
        })?;

        let mut content = match &choice.message.content {
            MessageContent::Text(text) => text.clone(),
            _ => "".to_string(),
        };

        // ä½¿ç”¨è¾…åŠ©æ¨¡å‹ä¼˜åŒ–å“åº”ï¼ˆå¦‚æœé…ç½®äº†è¾…åŠ©æ¨¡å‹åç§°ï¼‰
        if self.config.auxiliary_model_name.is_some() {
            info!("ä¸»æ¨¡å‹å“åº”æ— æ³•è§£æï¼Œä½¿ç”¨è¾…åŠ©æ¨¡å‹ä¿®æ­£");
            match self.send_auxiliary_request(&content).await {
                Ok(corrected_content) => {
                    content = corrected_content;
                },
                Err(e) => {
                    warn!("è¾…åŠ©æ¨¡å‹ä¿®æ­£å¤±è´¥: {}, ä½¿ç”¨åŸå§‹å“åº”", e);
                    // ç»§ç»­ä½¿ç”¨åŸå§‹å“åº”
                }
            }
        }

        let total_time = start_time.elapsed().as_secs_f64();

        // ä½¿ç”¨ AutoGLM ç‰¹æ®Šè§£æ
        let (thinking, parsed_actions) = self.parse_response(&content);

        let usage = chat_response.usage.unwrap_or(Usage {
            prompt_tokens: 0,
            completion_tokens: 0,
            total_tokens: 0,
        });

        // æ‰“å°æ€§èƒ½æŒ‡æ ‡
        info!("ğŸ“Š AutoGLM æ€§èƒ½æŒ‡æ ‡:");
        info!("   æ€»æ¨ç†æ—¶é—´: {:.3}s", total_time);
        info!("   ä½¿ç”¨ tokens: {}", usage.total_tokens);
        if let Some(ref t) = thinking {
            info!("   æ€è€ƒè¿‡ç¨‹: {}", t);
        }
        info!("   è§£æåˆ°çš„æ“ä½œæ•°: {}", parsed_actions.len());
        info!("   å®Œæ•´å“åº”: {}", &content);

        Ok(ModelResponse {
            content: content.clone(),
            actions: parsed_actions,
            confidence: 0.8,
            reasoning: thinking,
            tokens_used: usage.total_tokens,
        })
    }

    fn info(&self) -> ModelInfo {
        ModelInfo {
            name: self.config.model_name.clone(),
            provider: self.config.provider.clone(),
            supports_vision: true,
            max_tokens: self.config.max_tokens,
            context_window: 8192, // AutoGLM-Phone-9B çš„ä¸Šä¸‹æ–‡çª—å£
        }
    }

    fn supports_three_stage(&self) -> bool {
        self.config.enable_three_stage
    }

    fn set_logger(&self, logger: Option<std::sync::Arc<AgentLogger>>) {
        let mut logger_guard = self.logger.lock().unwrap();
        *logger_guard = logger;
    }
}

/// ChatResponse ç±»å‹ï¼ˆå¦‚æœæœªåœ¨ types.rs ä¸­å®šä¹‰ï¼‰
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatResponse {
    pub id: Option<String>,
    pub object: Option<String>,
    pub created: Option<u64>,
    pub model: Option<String>,
    pub choices: Vec<Choice>,
    pub usage: Option<Usage>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Choice {
    pub index: usize,
    pub message: ApiChatMessage,
    pub finish_reason: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Usage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::agent::core::traits::Action;

    #[test]
    fn test_parse_finish_action() {
        let client = AutoGLMClient::new(ModelConfig::default()).unwrap();
        let response = r#"Thinking...
finish(message="Task completed successfully")"#;

        let (thinking, actions) = client.parse_response(response);

        // éªŒè¯ action è§£ææˆåŠŸ
        assert!(!actions.is_empty());
        assert_eq!(actions.len(), 1);
        // éªŒè¯ action ç±»å‹ä¸º FinishAction
        assert_eq!(actions[0].action_type(), "finish");
        // thinking å¯èƒ½æ˜¯ Noneï¼ˆå› ä¸ºæ²¡æœ‰ <thinking> æ ‡ç­¾ï¼‰
        assert!(thinking.is_none() || thinking.as_ref().unwrap() == "Thinking...");
    }

    #[test]
    fn test_parse_do_action() {
        let client = AutoGLMClient::new(ModelConfig::default()).unwrap();
        let response = r#"Analyzing screen...
do(action="Tap", element=[500, 800])"#;

        let (thinking, actions) = client.parse_response(response);

        // éªŒè¯ thinking éƒ¨åˆ†ï¼ˆåº”è¯¥æ˜¯ Noneï¼Œå› ä¸ºæ²¡æœ‰ <thinking> æ ‡ç­¾ï¼‰
        assert!(thinking.is_none());

        // éªŒè¯ action è§£ææˆåŠŸ
        assert!(!actions.is_empty());
        assert_eq!(actions.len(), 1);
        // éªŒè¯ action ç±»å‹ä¸º TapAction
        assert_eq!(actions[0].action_type(), "tap");
    }

    #[test]
    fn test_parse_thinking_with_do() {
        let client = AutoGLMClient::new(ModelConfig::default()).unwrap();
        let response = r#"<thinking>I should tap the button at coordinates 100, 200</thinking>
do(action="Tap", element=[100, 200])"#;

        let (thinking, actions) = client.parse_response(response);

        // éªŒè¯ thinking éƒ¨åˆ†ï¼ˆä» <thinking> æ ‡ç­¾æå–ï¼‰
        assert_eq!(thinking, Some("I should tap the button at coordinates 100, 200".to_string()));

        // éªŒè¯ action è§£ææˆåŠŸ
        assert!(!actions.is_empty());
        assert_eq!(actions.len(), 1);
        // éªŒè¯ action ç±»å‹ä¸º TapAction
        assert_eq!(actions[0].action_type(), "tap");
    }

    #[test]
    fn test_parse_no_markers() {
        let client = AutoGLMClient::new(ModelConfig::default()).unwrap();
        let response = r#"Some random text without markers"#;

        let (thinking, actions) = client.parse_response(response);

        // thinking åº”è¯¥ä¸º Noneï¼ˆæ²¡æœ‰ <thinking> æ ‡ç­¾ï¼‰ï¼Œactions åº”è¯¥ä¸ºç©º
        assert!(thinking.is_none());
        assert!(actions.is_empty());
    }

    #[test]
    fn test_parse_priority() {
        let client = AutoGLMClient::new(ModelConfig::default()).unwrap();

        // finish(message= ä¼˜å…ˆçº§æœ€é«˜
        let response1 = r#"Text...
do(action=tap)
finish(message="done")"#;
        let (thinking, actions) = client.parse_response(response1);
        // thinking åº”è¯¥æ˜¯ Noneï¼ˆæ²¡æœ‰ <thinking> æ ‡ç­¾ï¼‰
        assert!(thinking.is_none());
        assert_eq!(actions.len(), 1);
        assert_eq!(actions[0].action_type(), "finish");

        // do(action= ç¬¬äºŒä¼˜å…ˆçº§
        let response2 = r#"<thinking>Thought</thinking>
<answer>answer content</answer>
do(action="Launch", app="å¾®ä¿¡")"#;
        let (thinking, actions) = client.parse_response(response2);
        // thinking åº”è¯¥æ˜¯ Some("Thought")
        assert_eq!(thinking, Some("Thought".to_string()));
        assert_eq!(actions.len(), 1);
        assert_eq!(actions[0].action_type(), "launch");
    }

    #[test]
    fn test_parse_do_action_launch() {
        let client = AutoGLMClient::new(ModelConfig::default()).unwrap();
        let response = r#"I need to open WeChat.
do(action="Launch", app="å¾®ä¿¡")"#;

        println!("Testing response: {:?}", response);
        let (thinking, actions) = client.parse_response(response);

        println!("Got thinking: {:?}", thinking);
        println!("Got actions: {:?}", actions);

        // thinking åº”è¯¥æ˜¯ Noneï¼ˆå› ä¸ºæ²¡æœ‰ <thinking> æ ‡ç­¾ï¼‰
        assert!(thinking.is_none());
        assert!(!actions.is_empty());
        // éªŒè¯ action ç±»å‹ä¸º LaunchAction
        assert_eq!(actions[0].action_type(), "launch");
    }

    #[test]
    fn test_parse_do_action_wait() {
        let client = AutoGLMClient::new(ModelConfig::default()).unwrap();
        let response = r#"åº”ç”¨æ­£åœ¨åŠ è½½ä¸­
do(action="Wait", duration=1, message="åº”ç”¨æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨ç­‰ã€‚")"#;

        let (thinking, actions) = client.parse_response(response);

        // thinking åº”è¯¥æ˜¯ Noneï¼ˆæ²¡æœ‰ <thinking> æ ‡ç­¾ï¼‰
        assert!(thinking.is_none());
        assert!(!actions.is_empty());
        // éªŒè¯ action ç±»å‹ä¸º WaitAction
        assert_eq!(actions[0].action_type(), "wait");
    }

    #[test]
    fn test_parse_finish_multiline() {
        let client = AutoGLMClient::new(ModelConfig::default()).unwrap();
        let response = r#"finish(message="æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°"ä»€ä¹ˆå€¼å¾—ä¹°"è¿™ä¸ªåº”ç”¨ã€‚

ä¸è¿‡ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æ‰“å¼€ä¸€äº›ç±»ä¼¼çš„åº”ç”¨æ¥æµè§ˆè´­ç‰©æˆ–æ¨èå†…å®¹ï¼Œæ¯”å¦‚ï¼š
- æ·˜å®
- ç¾å›¢

æ‚¨æƒ³æ‰“å¼€å“ªä¸ªåº”ç”¨æ¥æµè§ˆï¼Ÿ")"#;

        let (thinking, actions) = client.parse_response(response);

        // thinking åº”è¯¥æ˜¯ Noneï¼ˆæ²¡æœ‰ <thinking> æ ‡ç­¾ï¼‰
        assert!(thinking.is_none());
        assert!(!actions.is_empty());
        // éªŒè¯ action ç±»å‹ä¸º FinishAction
        assert_eq!(actions[0].action_type(), "finish");

        // éªŒè¯å¤šè¡Œæ¶ˆæ¯è¢«æ­£ç¡®è§£æ
        if let ActionEnum::Finish(ref finish) = actions[0] {
            assert!(finish.result.contains("æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°"));
            assert!(finish.result.contains("ä»€ä¹ˆå€¼å¾—ä¹°"));
            assert!(finish.result.contains("æ·˜å®"));
            assert!(finish.result.contains("ç¾å›¢"));
        } else {
            panic!("Expected FinishAction");
        }
    }
}
